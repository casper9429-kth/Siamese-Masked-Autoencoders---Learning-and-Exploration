model_name: siamMAE
model_class: model:SiamMAE
optimizer: AdamW # Hard coded in src/pretraining.py
optimizer_momentum:
    beta1: 0.9
    beta2: 0.95
weight_decay: 5.0e-2 
base_learning_rate: 1.0e-3
learning_rate: 1.5e-4
min_learning_rate: 0.0
learning_rate_scheduler: cosine decay # Hard coded in src/pretraining.py
warmup_epochs: 40
epochs: 2000 
epochs_ablation: 400
check_val_every_n_epoch: 1
ablation: False
jax_disable_jit: False 
mask_ratio: 0.95
random_seed: 42
dataset: data:PreTrainingDataset
CHECKPOINT_PATH: "../checkpoints/" 
augmentation:   # Proably hard coded
    - hflip
    - crop:
        - 0.5
        - 1
batch_size: 256 # (2048)Probably hard coded
repeted_sampling: 2
frame_sampling_gap: # Probably hard coded
    - 4
    - 48
model_param: # Probably hard coded
    img_size : 224
    patch_size : 32
    in_chans : 3
    embed_dim : 768
    depth : 12
    encoder_hidden_dim : 1
    num_heads : 12
    decoder_embed_dim : 512
    decoder_depth : 8
    decoder_hidden_dim : 1
    decoder_num_heads : 16
    mask_ratio : 0.95