model_name: siamMAE
model_class: model:SiamMAE
optimizer_momentum:
    beta1: 0.9
    beta2: 0.95
weight_decay:  5.0e-2  # 5.0e-2 (paper) 
base_learning_rate: 1.0e-4 # paper: 1.0e-3
learning_rate: 1.0e-5 # paper: 1.5e-4
min_learning_rate: 0.0
warmup_epochs: 5
epochs: 500
jax_disable_jit: False 
random_seed: 42
dataset: data:PreTrainingDataset
CHECKPOINT_PATH: /home/casper9429/repos/Siamese-Masked-Autoencoders---Learning-and-Exploration/checkpoints/
dataset_path: ./data/Kinetics/train_jpg/*
start_checkpoint: 
    start_from_checkpoint: True
    checkpoint_path: /home/casper9429/repos/Siamese-Masked-Autoencoders---Learning-and-Exploration/checkpoints_save/final_30_epochs/
test_batch_size: 1
repeted_sampling: 2 # 2 (paper)
batch_size: 40 # (2048)
test_on_validation: True
test_dataset_path: ./data/Kinetics/train_jpg_test/*
log_images:
    log_images: True
    nr_images_to_log_per_epoch: 2
frame_sampling_gap: # Probably hard coded
    - 2
    - 10
augmentation:   # Proably hard coded
    hflip: 0.5
    crop:
        - 0.5
        - 1
save_model_interval: 10
early_stopping_threshold: 0.01
model_param: # Probably hard coded
    img_size : 224
    patch_size : 16
    in_chans : 3
    embed_dim : 768
    depth : 12
    encoder_hidden_dim : 3072 # int(4*768)
    num_heads : 12
    decoder_embed_dim : 512
    decoder_depth : 8
    decoder_hidden_dim : 2048 # int(4*512)
    decoder_num_heads : 16
    mask_ratio : 0.95